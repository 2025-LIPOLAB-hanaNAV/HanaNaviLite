# HanaNaviLite ê°œë°œ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” HanaNaviLite í”„ë¡œì íŠ¸ì˜ ê°œë°œ ì§€ì¹¨ê³¼ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ¯ **ê°œë°œ ì² í•™**

### **1. ê²½ëŸ‰í™” ìš°ì„ **
- 32GB RAM ì œì•½ í•˜ì—ì„œ ìµœì  ì„±ëŠ¥ ë‹¬ì„±
- ë¶ˆí•„ìš”í•œ ì˜ì¡´ì„± ìµœì†Œí™”
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì§€ì†ì  ëª¨ë‹ˆí„°ë§

### **2. ìì²´ êµ¬í˜„ ì¤‘ì‹¬**
- í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì€ ì§ì ‘ ê°œë°œ
- ì˜¤í”ˆì†ŒìŠ¤ëŠ” ì¸í”„ë¼ ë ˆë²¨ì—ì„œë§Œ í™œìš©
- í•œêµ­ì–´ íŠ¹í™” ì²˜ë¦¬ ë¡œì§ ê°•í™”

### **3. ì‹¤ìš©ì„± ì¤‘ì‹¬**
- MVP ê´€ì ì—ì„œ í•„ìˆ˜ ê¸°ëŠ¥ë§Œ êµ¬í˜„
- í™•ì¥ ê°€ëŠ¥í•œ ëª¨ë“ˆ ì„¤ê³„
- ì€í–‰ ì—…ë¬´ íŠ¹ì„± ë°˜ì˜

---

## ğŸ—ï¸ **ì•„í‚¤í…ì²˜ ì„¤ê³„**

### **ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°**

```mermaid
graph TB
    UI[React Frontend] --> Gateway[FastAPI Gateway]
    Gateway --> Search[Hybrid Search Engine]
    Gateway --> ETL[ETL Pipeline]
    Gateway --> LLM[LLM Service]
    
    Search --> SQLite[(SQLite DB)]
    Search --> FAISS[(FAISS Index)]
    
    ETL --> Parser[File Parser]
    ETL --> Embed[Embedding Service]
    
    LLM --> Ollama[Ollama Server]
```

### **ë°ì´í„° í”Œë¡œìš°**

1. **ì¸ë±ì‹± í”Œë¡œìš°**
   ```
   ê²Œì‹œê¸€ ì›¹í›… â†’ ETL Pipeline â†’ íŒŒì¼ íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ FAISS/SQLite ì €ì¥
   ```

2. **ê²€ìƒ‰ í”Œë¡œìš°**
   ```
   ì‚¬ìš©ì ì§ˆì˜ â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â†’ RRF ìœµí•© â†’ ì¬ë­í‚¹ â†’ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
   ```

3. **ì‘ë‹µ í”Œë¡œìš°**
   ```
   ì»¨í…ìŠ¤íŠ¸ + ì§ˆì˜ â†’ LLM ì¶”ë¡  â†’ ë‹µë³€ ìƒì„± â†’ í›„ì²˜ë¦¬ â†’ ì‚¬ìš©ì ì‘ë‹µ
   ```

---

## ğŸ”§ **í•µì‹¬ ëª¨ë“ˆ ì„¤ê³„**

### **1. FastAPI Gateway (`app/core/`)**

**ì—­í• **: ëª¨ë“  ìš”ì²­ì˜ ì§„ì…ì  ë° ë¼ìš°íŒ…
**ì£¼ìš” ê¸°ëŠ¥**:
- API ë¼ìš°íŒ… ë° ìš”ì²­ ì²˜ë¦¬
- ì¸ì¦/ê¶Œí•œ ê´€ë¦¬ (ì„ íƒ)
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…
- í—¬ìŠ¤ì²´í¬ ë° ëª¨ë‹ˆí„°ë§

```python
# app/core/main.py
from fastapi import FastAPI
from .routes import search, rag, etl, health

app = FastAPI(title="HanaNaviLite")

app.include_router(search.router, prefix="/search")
app.include_router(rag.router, prefix="/rag") 
app.include_router(etl.router, prefix="/etl")
app.include_router(health.router, prefix="/health")
```

### **2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ (`app/search/`)**

**ì—­í• **: IR + Vector í†µí•© ê²€ìƒ‰
**ì£¼ìš” ê¸°ëŠ¥**:
- SQLite FTS5 ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰
- FAISS ê¸°ë°˜ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰  
- RRF ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ê²°ê³¼ ìœµí•©
- í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”

```python
# app/search/hybrid_engine.py
class HybridSearchEngine:
    def __init__(self):
        self.sqlite_searcher = SQLiteSearcher()
        self.faiss_searcher = FAISSSearcher()
        self.korean_processor = KoreanTextProcessor()
    
    def search(self, query: str, top_k: int = 20) -> List[SearchResult]:
        # 1. í•œêµ­ì–´ ì¿¼ë¦¬ ì •ê·œí™”
        normalized_query = self.korean_processor.normalize(query)
        
        # 2. ë³‘ë ¬ ê²€ìƒ‰
        ir_results = self.sqlite_searcher.search(normalized_query, top_k)
        vector_results = self.faiss_searcher.search(query, top_k)
        
        # 3. RRF ìœµí•©
        return self.rrf_fusion(ir_results, vector_results, top_k)
```

### **3. ETL íŒŒì´í”„ë¼ì¸ (`app/etl/`)**

**ì—­í• **: ë°ì´í„° ìˆ˜ì§‘, ë³€í™˜, ì ì¬
**ì£¼ìš” ê¸°ëŠ¥**:
- ì›¹í›… ê¸°ë°˜ ê²Œì‹œê¸€ ìˆ˜ì§‘
- ë‹¤ì¤‘ íŒŒì¼ í˜•ì‹ íŒŒì‹± (PDF/XLSX/DOCX)
- í…ìŠ¤íŠ¸ ì²­í‚¹ ë° ì„ë² ë”©
- SQLite/FAISS ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸

```python
# app/etl/pipeline.py
class ETLPipeline:
    def process_webhook(self, webhook_data: dict):
        # 1. ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        post_data = self.extract_post_metadata(webhook_data)
        
        # 2. ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±
        documents = self.parse_attachments(post_data.attachments)
        
        # 3. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = self.chunk_documents(documents)
        
        # 4. ì„ë² ë”© ìƒì„± ë° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self.update_indexes(chunks)
```

### **4. LLM ì„œë¹„ìŠ¤ (`app/llm/`)**

**ì—­í• **: ì–¸ì–´ëª¨ë¸ ì¶”ë¡  ë° ë‹µë³€ ìƒì„±
**ì£¼ìš” ê¸°ëŠ¥**:
- Ollama ì„œë²„ í†µì‹ 
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
- ë‹µë³€ í’ˆì§ˆ í›„ì²˜ë¦¬

```python
# app/llm/service.py
class LLMService:
    def generate_answer(self, query: str, context: List[str]) -> str:
        # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.build_prompt(query, context)
        
        # 2. LLM ì¶”ë¡ 
        response = self.ollama_client.generate(prompt)
        
        # 3. ë‹µë³€ í›„ì²˜ë¦¬
        return self.post_process_answer(response)
```

---

## ğŸ’¾ **ë°ì´í„° ê´€ë¦¬**

### **SQLite í†µí•© ìŠ¤í‚¤ë§ˆ**

```sql
-- ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„°
CREATE TABLE posts (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT,
    category TEXT,
    posted_at DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- ë¬¸ì„œ ì²­í¬
CREATE TABLE chunks (
    id TEXT PRIMARY KEY,
    post_id INTEGER,
    chunk_text TEXT NOT NULL,
    chunk_index INTEGER,
    metadata JSON,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);

-- FTS5 ê²€ìƒ‰ ì¸ë±ìŠ¤
CREATE VIRTUAL TABLE chunks_fts USING fts5(
    chunk_id UNINDEXED,
    content,
    category,
    tokenize='unicode61'
);

-- ìºì‹œ í…Œì´ë¸” (Redis ëŒ€ì²´)
CREATE TABLE cache (
    key TEXT PRIMARY KEY,
    value BLOB,
    expires_at DATETIME
);

-- ì„ë² ë”© ìºì‹œ
CREATE TABLE embedding_cache (
    text_hash TEXT PRIMARY KEY,
    embedding BLOB,
    model_name TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

### **FAISS ì¸ë±ìŠ¤ ê´€ë¦¬**

```python
# app/search/faiss_manager.py
class FAISSManager:
    def __init__(self, dimension: int = 1024):
        self.dimension = dimension
        self.index = faiss.IndexFlatIP(dimension)  # Inner Product
        self.id_map = {}  # chunk_id â†’ faiss_id ë§¤í•‘
    
    def add_vectors(self, chunk_ids: List[str], vectors: np.ndarray):
        """ë²¡í„° ì¶”ê°€"""
        start_id = self.index.ntotal
        self.index.add(vectors)
        
        # ID ë§¤í•‘ ì—…ë°ì´íŠ¸
        for i, chunk_id in enumerate(chunk_ids):
            self.id_map[chunk_id] = start_id + i
    
    def search(self, query_vector: np.ndarray, top_k: int) -> List[Tuple[str, float]]:
        """ë²¡í„° ê²€ìƒ‰"""
        scores, faiss_ids = self.index.search(query_vector.reshape(1, -1), top_k)
        
        results = []
        for faiss_id, score in zip(faiss_ids[0], scores[0]):
            chunk_id = self.reverse_id_map.get(faiss_id)
            if chunk_id:
                results.append((chunk_id, float(score)))
        
        return results
```

---

## ğŸ§ª **í…ŒìŠ¤íŠ¸ ì „ëµ**

### **1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**
```bash
# ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸
pytest tests/test_search_engine.py -v

# ETL íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸  
pytest tests/test_etl_pipeline.py -v

# LLM ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
pytest tests/test_llm_service.py -v
```

### **2. í†µí•© í…ŒìŠ¤íŠ¸**
```bash
# API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
pytest tests/test_api_integration.py -v

# ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
pytest tests/test_rag_pipeline.py -v
```

### **3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**
```python
# tests/test_performance.py
def test_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 25GB ì´í•˜ í™•ì¸"""
    memory_usage = get_memory_usage()
    assert memory_usage < 25 * 1024 * 1024 * 1024  # 25GB

def test_response_time():
    """ì‘ë‹µ ì‹œê°„ 3ì´ˆ ì´í•˜ í™•ì¸"""
    start_time = time.time()
    response = client.post("/rag/query", json={"query": "í…ŒìŠ¤íŠ¸ ì§ˆì˜"})
    elapsed = time.time() - start_time
    assert elapsed < 3.0
```

---

## ğŸ”„ **CI/CD íŒŒì´í”„ë¼ì¸**

### **GitHub Actions ì›Œí¬í”Œë¡œìš°**

```yaml
# .github/workflows/test.yml
name: Test Pipeline

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run tests
        run: pytest tests/ -v --cov=app
      
      - name: Check code quality
        run: |
          black --check app/
          isort --check app/
          flake8 app/
```

---

## ğŸ“Š **ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…**

### **ë©”íŠ¸ë¦­ ìˆ˜ì§‘**
```python
# app/utils/monitoring.py
import psutil
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        result = func(*args, **kwargs)
        
        elapsed_time = time.time() - start_time
        memory_used = psutil.virtual_memory().used - start_memory
        
        logger.info(f"{func.__name__}: {elapsed_time:.2f}s, {memory_used/1024/1024:.1f}MB")
        return result
    return wrapper
```

### **êµ¬ì¡°í™”ëœ ë¡œê¹…**
```python
# app/utils/logging.py
import structlog

logger = structlog.get_logger()

# ì‚¬ìš© ì˜ˆì‹œ
logger.info("RAG query processed", 
           query=query, 
           response_time=elapsed_time,
           memory_usage=memory_usage,
           num_sources=len(sources))
```

---

## ğŸš€ **ë°°í¬ ê°€ì´ë“œ**

### **Docker ìµœì í™”**
```dockerfile
# Dockerfile
FROM python:3.11-slim as base

# ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œë¡œ ì´ë¯¸ì§€ í¬ê¸° ìµœì†Œí™”
FROM base as builder
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM base as runtime
COPY --from=builder /root/.local /root/.local
COPY app/ /app/
ENV PATH=/root/.local/bin:$PATH

# ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
ENV PYTHONMALLOC=malloc
ENV MALLOC_TRIM_THRESHOLD_=100000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8001"]
```

### **Docker Compose ì„¤ì •**
```yaml
# docker-compose.yml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8001:8001"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
  
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  ollama_data:
```

---

## ğŸ”§ **ê°œë°œ ë„êµ¬**

### **í•„ìˆ˜ ë„êµ¬**
- **Poetry**: ì˜ì¡´ì„± ê´€ë¦¬
- **Black**: ì½”ë“œ í¬ë§¤íŒ…
- **isort**: import ì •ë ¬
- **flake8**: ë¦°íŒ…
- **pytest**: í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
- **mypy**: íƒ€ì… ì²´í‚¹

### **ê¶Œì¥ VS Code í™•ì¥**
- Python
- Pylance  
- Python Docstring Generator
- GitLens
- Docker
- REST Client

### **ê°œë°œ ëª…ë ¹ì–´**
```bash
# ì½”ë“œ í’ˆì§ˆ ì²´í¬
make lint

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
make test

# ê°œë°œ ì„œë²„ ì‹¤í–‰
make dev

# Docker ë¹Œë“œ
make build

# ì „ì²´ CI íŒŒì´í”„ë¼ì¸ ë¡œì»¬ ì‹¤í–‰
make ci
```

---

## ğŸ“ **ì½”ë”© ì»¨ë²¤ì…˜**

### **Python ìŠ¤íƒ€ì¼**
- **PEP 8** ì¤€ìˆ˜
- **íƒ€ì… íŒíŠ¸** í•„ìˆ˜
- **Docstring** í•„ìˆ˜ (Google ìŠ¤íƒ€ì¼)
- **í•¨ìˆ˜ëª…**: snake_case
- **í´ë˜ìŠ¤ëª…**: PascalCase
- **ìƒìˆ˜ëª…**: UPPER_CASE

### **ì»¤ë°‹ ë©”ì‹œì§€**
```
type(scope): subject

body

footer
```

**ì˜ˆì‹œ:**
```
feat(search): add hybrid search engine

- Implement SQLite FTS5 + FAISS integration
- Add RRF fusion algorithm
- Support Korean text normalization

Resolves #123
```

---

## ğŸ¯ **ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ**

### **ë©”ëª¨ë¦¬ ìµœì í™”**
1. **FAISS ì¸ë±ìŠ¤ ìµœì í™”**
   - IndexIVFFlat ì‚¬ìš© ê³ ë ¤
   - ì£¼ê¸°ì  ì¸ë±ìŠ¤ ì••ì¶•

2. **SQLite ìµœì í™”**
   - WAL ëª¨ë“œ í™œì„±í™”
   - ì ì ˆí•œ ìºì‹œ í¬ê¸° ì„¤ì •
   - ì¸ë±ìŠ¤ ìµœì í™”

3. **Python ë©”ëª¨ë¦¬ ê´€ë¦¬**
   - ëŒ€ìš©ëŸ‰ ê°ì²´ ì¦‰ì‹œ í•´ì œ
   - ì œë„ˆë ˆì´í„° í™œìš©
   - ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

### **ì‘ë‹µ ì‹œê°„ ìµœì í™”**
1. **ê²€ìƒ‰ ìµœì í™”**
   - ë³‘ë ¬ ê²€ìƒ‰ ì²˜ë¦¬
   - ìºì‹± í™œìš©
   - ì¡°ê¸° ì¢…ë£Œ ë¡œì§

2. **LLM ìµœì í™”**
   - ëª¨ë¸ ì›Œë°ì—…
   - ë°°ì¹˜ ì²˜ë¦¬
   - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

---

ì´ ê°œë°œ ê°€ì´ë“œëŠ” HanaNaviLite í”„ë¡œì íŠ¸ì˜ ì„±ê³µì ì¸ ê°œë°œê³¼ ìš´ì˜ì„ ìœ„í•œ í•µì‹¬ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤. ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ê°œì„ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.