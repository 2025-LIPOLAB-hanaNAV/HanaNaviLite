
services:
  # Ollama + Gemma3 서비스
  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-with-gemma3
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - NVIDIA_VISIBLE_DEVICES=all
    networks:
      - lipolab-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # HanaNaviLite RAG 백엔드 서비스
  rag-backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: hananavilite-rag-backend
    ports:
      - "3030:3030"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./uploads:/app/uploads
      - ./logs:/app/logs
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - LLM_MODEL=gemma3:12b-it-qat
      - DATABASE_URL=sqlite:///data/hananavilite.db
      - LOG_LEVEL=INFO
      - NVIDIA_VISIBLE_DEVICES=all
      - API_PORT=3030
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - lipolab-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # UI 챗봇 프론트엔드 서비스
  ui-chatbot-frontend:
    build:
      context: ./ui/chatbot-react
      dockerfile: Dockerfile
    container_name: hananavilite-ui-frontend
    ports:
      - "8080:80"
    environment:
      - REACT_APP_API_BASE_URL=http://rag-backend:3030
    depends_on:
      - rag-backend
    networks:
      - lipolab-network
    restart: unless-stopped

  # PII-GUARD 서비스 (기존)
  pii-guard:
    build:
      context: ./PII-GUARD
      dockerfile: Dockerfile
    container_name: lipolab-pii-guard
    ports:
      - "3000:3000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=gemma3:12b-it-qat
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - lipolab-network
    restart: unless-stopped
    volumes:
      - ./pii-guard/data:/app/data

  # 기존 백엔드 서비스
  backend:
    build:
      context: ./web-portal-backend
      dockerfile: Dockerfile
    container_name: lipolab-backend
    ports:
      - "3001:3001"
    volumes:
      - ./web-portal-backend/data:/app/data
    environment:
      - PORT=3001
      - PII_GUARD_URL=http://pii-guard:3000
      - OLLAMA_URL=http://ollama:11434
      - RAG_BACKEND_URL=http://rag-backend:3030
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "3001"]
    depends_on:
      - pii-guard
      - rag-backend
    networks:
      - lipolab-network
    restart: unless-stopped

  # 기존 프론트엔드 서비스
  frontend:
    build:
      context: ./web-portal-mockup
      dockerfile: Dockerfile
    container_name: lipolab-frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    networks:
      - lipolab-network
    restart: unless-stopped

volumes:
  ollama_data:
    driver: local
  huggingface_cache:
    driver: local

networks:
  lipolab-network:
    driver: bridge
