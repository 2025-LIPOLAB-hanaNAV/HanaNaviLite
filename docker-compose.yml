version: '3.8'

services:
  # HanaNaviLite 메인 애플리케이션
  hananavilite:
    build: .
    container_name: hananavilite-app
    ports:
      - "8001:8001"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11435  # 로컬 Ollama 사용시 (현재 11435 포트)
      # - OLLAMA_BASE_URL=http://ollama:11434  # Ollama 컨테이너 사용시
      - EMBEDDING_MODEL=dragonkue/snowflake-arctic-embed-l-v2.0-ko
      - LLM_MODEL=gemma3:12b-it-qat
      - DATABASE_URL=sqlite:///data/hananavilite.db
      - LOG_LEVEL=INFO
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - hananavilite-network

  # Redis 캐시 서버 (선택적)
  redis:
    image: redis:7-alpine
    container_name: hananavilite-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - hananavilite-network

  # Ollama 서버 (선택적 - 로컬 Ollama 대신 사용 가능)
  ollama:
    image: ollama/ollama:latest
    container_name: hananavilite-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - hananavilite-network
    profiles:
      - ollama-container  # 이 프로파일로 선택적 실행

volumes:
  redis_data:
  ollama_data:

networks:
  hananavilite-network:
    driver: bridge